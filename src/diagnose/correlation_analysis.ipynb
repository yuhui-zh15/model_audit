{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from datasets import ImageDataset, TextDataset, create_dataloader\n",
    "from models import Linear\n",
    "from trainer import run_one_epoch\n",
    "from utils import computing_subgroup_metrics, subgrouping\n",
    "from prepare_text_datasets import prepare_waterbird, prepare_fairface, prepare_dspites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(\n",
    "    model: torch.nn.Module,\n",
    "    clip_model: torch.nn.Module,\n",
    "    transform: torchvision.transforms,\n",
    "    image_data: List[dict],\n",
    "    text_data: List[dict],\n",
    ") -> Tuple[dict, dict]:\n",
    "    image_dataset = ImageDataset(data=image_data)\n",
    "    image_dataloader = create_dataloader(\n",
    "        dataset=image_dataset, modality=\"image\", transform=transform\n",
    "    )\n",
    "    image_metrics = run_one_epoch(\n",
    "        dataloader=image_dataloader,\n",
    "        model=model,\n",
    "        clip_model=clip_model,\n",
    "        modality=\"image\",\n",
    "        opt=None,\n",
    "        epoch_idx=-1,\n",
    "        eval=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    text_dataset = TextDataset(data=text_data)\n",
    "    text_dataloader = create_dataloader(dataset=text_dataset, modality=\"text\")\n",
    "    text_metrics = run_one_epoch(\n",
    "        dataloader=text_dataloader,\n",
    "        model=model,\n",
    "        clip_model=clip_model,\n",
    "        modality=\"text\",\n",
    "        opt=None,\n",
    "        epoch_idx=-1,\n",
    "        eval=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return image_metrics, text_metrics\n",
    "\n",
    "\n",
    "def compute_correlation(\n",
    "    data1_list: List, data2_list: List, visualization: bool = False\n",
    ") -> None:\n",
    "    assert len(data1_list) == len(data2_list)\n",
    "    data1 = np.array(data1_list)\n",
    "    data2 = np.array(data2_list)\n",
    "    spearmanr_corr, spearmanr_pval = spearmanr(data1, data2)\n",
    "    pearsonr_corr, pearsonr_pval = pearsonr(data1, data2)\n",
    "    print(f\"Spearman correlation: {spearmanr_corr:.4f} (p-value: {spearmanr_pval:.4f})\")\n",
    "    print(f\"Pearson correlation: {pearsonr_corr:.4f} (p-value: {pearsonr_pval:.4f})\")\n",
    "    if visualization:\n",
    "        plt.scatter(data1, data2)\n",
    "        plt.xlabel(\"Image\")\n",
    "        plt.ylabel(\"Text\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compute_subgroup_correlation(\n",
    "    image_data: List,\n",
    "    image_metrics: List,\n",
    "    text_data: List,\n",
    "    text_metrics: List,\n",
    "    fields: List[str],\n",
    "    visualization: bool = False,\n",
    ") -> None:\n",
    "    image_subgroups = subgrouping(image_data, fields)\n",
    "    image_instance_accs = np.array(image_metrics[\"preds\"]) == np.array(\n",
    "        image_metrics[\"labels\"]\n",
    "    )\n",
    "    image_subgroup_accs = computing_subgroup_metrics(\n",
    "        image_instance_accs, image_subgroups\n",
    "    )\n",
    "\n",
    "    text_subgroups = subgrouping(text_data, fields)\n",
    "    text_instance_accs = np.array(text_metrics[\"preds\"]) == np.array(\n",
    "        text_metrics[\"labels\"]\n",
    "    )\n",
    "    text_subgroup_accs = computing_subgroup_metrics(text_instance_accs, text_subgroups)\n",
    "\n",
    "    text_instance_probs = torch.softmax(\n",
    "        torch.tensor(text_metrics[\"logits\"]), dim=1\n",
    "    ).numpy()[np.arange(len(text_metrics[\"labels\"])), text_metrics[\"labels\"]]\n",
    "    text_subgroup_probs = computing_subgroup_metrics(\n",
    "        text_instance_probs, text_subgroups\n",
    "    )\n",
    "\n",
    "    print(\"Text Acc - Image Acc Correlation:\")\n",
    "    compute_correlation(\n",
    "        [text_subgroup_accs[x] for x in image_subgroups],\n",
    "        [image_subgroup_accs[x] for x in image_subgroups],\n",
    "    )\n",
    "    print(\"Text Prob - Image Acc Correlation:\")\n",
    "    compute_correlation(\n",
    "        [text_subgroup_probs[x] for x in image_subgroups],\n",
    "        [image_subgroup_accs[x] for x in image_subgroups],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waterbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concat:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.4167 (p-value: 0.0000)\n",
      "Pearson correlation: 0.4355 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.5899 (p-value: 0.0000)\n",
      "Pearson correlation: 0.5773 (p-value: 0.0000)\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.5607 (p-value: 0.0000)\n",
      "Pearson correlation: 0.5742 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.6462 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6721 (p-value: 0.0000)\n",
      "\n",
      "Ensemble:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.6704 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6091 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.6465 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6776 (p-value: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "CLIP_MODEL = \"ViT-B/32\"\n",
    "LINEAR_MODEL = \"../pytorch_cache/iclrsubmission/models/waterbird_linear_model.pt\"\n",
    "DATA_PATH = \"../../data/Waterbird/processed_attribute_dataset/attributes.jsonl\"\n",
    "FIELDS = [\"species\", \"place\"]\n",
    "N_CLASS = 2\n",
    "\n",
    "clip_model, transform = clip.load(name=CLIP_MODEL, device=\"cuda\")\n",
    "clip_model = clip_model.float()\n",
    "model = Linear(clip_model.visual.output_dim, N_CLASS).cuda()\n",
    "model.load_state_dict(torch.load(LINEAR_MODEL))\n",
    "\n",
    "\n",
    "def filter_fn(x):\n",
    "    return x[\"attributes\"][\"split\"] == \"val\"\n",
    "\n",
    "\n",
    "def label_fn(x):\n",
    "    return x[\"attributes\"][\"waterbird\"]\n",
    "\n",
    "\n",
    "image_data = [json.loads(line) for line in open(DATA_PATH)]\n",
    "image_data = [x for x in image_data if filter_fn(x)]\n",
    "for item in image_data:\n",
    "    item[\"label\"] = label_fn(item)\n",
    "\n",
    "text_data_concat = prepare_waterbird(data_path=DATA_PATH, input_type=\"concat\")\n",
    "text_data_prompt = prepare_waterbird(data_path=DATA_PATH, input_type=\"prompt\")\n",
    "text_data_ensemble = prepare_waterbird(data_path=DATA_PATH, input_type=\"ensemble\")\n",
    "\n",
    "print(\"\\nConcat:\\n\")\n",
    "image_metrics, text_metrics_concat = get_model_output(\n",
    "    model, clip_model, transform, image_data, text_data_concat\n",
    ")\n",
    "compute_subgroup_correlation(\n",
    "    image_data, image_metrics, text_data_concat, text_metrics_concat, fields=FIELDS\n",
    ")\n",
    "\n",
    "print(\"\\nPrompt:\\n\")\n",
    "image_metrics, text_metrics_prompt = get_model_output(\n",
    "    model, clip_model, transform, image_data, text_data_prompt\n",
    ")\n",
    "compute_subgroup_correlation(\n",
    "    image_data, image_metrics, text_data_prompt, text_metrics_prompt, fields=FIELDS\n",
    ")\n",
    "\n",
    "print(\"\\nEnsemble:\\n\")\n",
    "image_metrics, text_metrics_ensemble = get_model_output(\n",
    "    model, clip_model, transform, image_data, text_data_ensemble\n",
    ")\n",
    "compute_subgroup_correlation(\n",
    "    image_data, image_metrics, text_data_ensemble, text_metrics_ensemble, fields=FIELDS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dalle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf49421d02fb18daac2fe024769d7389ca36bccb970e26253e571efb021ca22f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
