{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple, Callable\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from datasets import ImageDataset, TextDataset, create_dataloader\n",
    "from models import Linear\n",
    "from trainer import run_one_epoch\n",
    "from utils import computing_subgroup_metrics, subgrouping\n",
    "from prepare_text_datasets import prepare_waterbird, prepare_fairface, prepare_dspites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_output(\n",
    "    model: torch.nn.Module,\n",
    "    clip_model: torch.nn.Module,\n",
    "    transform: torchvision.transforms,\n",
    "    image_data: List[dict],\n",
    "    text_data: List[dict],\n",
    ") -> Tuple[dict, dict]:\n",
    "    image_dataset = ImageDataset(data=image_data)\n",
    "    image_dataloader = create_dataloader(\n",
    "        dataset=image_dataset, modality=\"image\", transform=transform\n",
    "    )\n",
    "    image_metrics = run_one_epoch(\n",
    "        dataloader=image_dataloader,\n",
    "        model=model,\n",
    "        clip_model=clip_model,\n",
    "        modality=\"image\",\n",
    "        opt=None,\n",
    "        epoch_idx=-1,\n",
    "        eval=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    text_dataset = TextDataset(data=text_data)\n",
    "    text_dataloader = create_dataloader(dataset=text_dataset, modality=\"text\")\n",
    "    text_metrics = run_one_epoch(\n",
    "        dataloader=text_dataloader,\n",
    "        model=model,\n",
    "        clip_model=clip_model,\n",
    "        modality=\"text\",\n",
    "        opt=None,\n",
    "        epoch_idx=-1,\n",
    "        eval=True,\n",
    "        verbose=False,\n",
    "    )\n",
    "    return image_metrics, text_metrics\n",
    "\n",
    "\n",
    "def compute_correlation(\n",
    "    data1_list: List, data2_list: List, visualization: bool = False\n",
    ") -> None:\n",
    "    assert len(data1_list) == len(data2_list)\n",
    "    data1 = np.array(data1_list)\n",
    "    data2 = np.array(data2_list)\n",
    "    spearmanr_corr, spearmanr_pval = spearmanr(data1, data2)\n",
    "    pearsonr_corr, pearsonr_pval = pearsonr(data1, data2)\n",
    "    print(f\"Spearman correlation: {spearmanr_corr:.4f} (p-value: {spearmanr_pval:.4f})\")\n",
    "    print(f\"Pearson correlation: {pearsonr_corr:.4f} (p-value: {pearsonr_pval:.4f})\")\n",
    "    if visualization:\n",
    "        plt.scatter(data1, data2)\n",
    "        plt.xlabel(\"Image\")\n",
    "        plt.ylabel(\"Text\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def compute_subgroup_correlation(\n",
    "    image_data: List,\n",
    "    image_metrics: List,\n",
    "    text_data: List,\n",
    "    text_metrics: List,\n",
    "    fields: List[str],\n",
    "    visualization: bool = False,\n",
    ") -> None:\n",
    "    image_subgroups = subgrouping(image_data, fields)\n",
    "    image_instance_accs = np.array(image_metrics[\"preds\"]) == np.array(\n",
    "        image_metrics[\"labels\"]\n",
    "    )\n",
    "    image_subgroup_accs = computing_subgroup_metrics(\n",
    "        image_instance_accs, image_subgroups\n",
    "    )\n",
    "\n",
    "    text_subgroups = subgrouping(text_data, fields)\n",
    "    text_instance_accs = np.array(text_metrics[\"preds\"]) == np.array(\n",
    "        text_metrics[\"labels\"]\n",
    "    )\n",
    "    text_subgroup_accs = computing_subgroup_metrics(text_instance_accs, text_subgroups)\n",
    "\n",
    "    text_instance_probs = torch.softmax(\n",
    "        torch.tensor(text_metrics[\"logits\"]), dim=1\n",
    "    ).numpy()[np.arange(len(text_metrics[\"labels\"])), text_metrics[\"labels\"]]\n",
    "    text_subgroup_probs = computing_subgroup_metrics(\n",
    "        text_instance_probs, text_subgroups\n",
    "    )\n",
    "\n",
    "    print(\"Text Acc - Image Acc Correlation:\")\n",
    "    compute_correlation(\n",
    "        [text_subgroup_accs[x] for x in image_subgroups],\n",
    "        [image_subgroup_accs[x] for x in image_subgroups],\n",
    "    )\n",
    "    print(\"Text Prob - Image Acc Correlation:\")\n",
    "    compute_correlation(\n",
    "        [text_subgroup_probs[x] for x in image_subgroups],\n",
    "        [image_subgroup_accs[x] for x in image_subgroups],\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_dataset_correlation(\n",
    "    clip_model_name: str,\n",
    "    linear_model_path: str,\n",
    "    data_path: str,\n",
    "    filter_fn: Callable,\n",
    "    label_fn: Callable,\n",
    "    prepare_fn: Callable,\n",
    "    fields: List[str],\n",
    ") -> None:\n",
    "    clip_model, transform = clip.load(name=clip_model_name, device=\"cuda\")\n",
    "    clip_model = clip_model.float()\n",
    "    state_dict = torch.load(linear_model_path)\n",
    "    n_class = state_dict[\"fc.weight\"].shape[0]\n",
    "    model = Linear(clip_model.visual.output_dim, n_class).cuda()\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    image_data = [json.loads(line) for line in open(data_path)]\n",
    "    image_data = [item for idx, item in enumerate(image_data) if filter_fn(idx, item)]\n",
    "    for item in image_data:\n",
    "        item[\"label\"] = label_fn(item)\n",
    "\n",
    "    text_data_concat = prepare_fn(data_path=data_path, input_type=\"concat\")\n",
    "    text_data_prompt = prepare_fn(data_path=data_path, input_type=\"prompt\")\n",
    "    text_data_ensemble = prepare_fn(data_path=data_path, input_type=\"ensemble\")\n",
    "\n",
    "    print(\"\\nConcat:\\n\")\n",
    "    image_metrics, text_metrics_concat = get_model_output(\n",
    "        model, clip_model, transform, image_data, text_data_concat\n",
    "    )\n",
    "    compute_subgroup_correlation(\n",
    "        image_data, image_metrics, text_data_concat, text_metrics_concat, fields=fields\n",
    "    )\n",
    "\n",
    "    print(\"\\nPrompt:\\n\")\n",
    "    image_metrics, text_metrics_prompt = get_model_output(\n",
    "        model, clip_model, transform, image_data, text_data_prompt\n",
    "    )\n",
    "    compute_subgroup_correlation(\n",
    "        image_data, image_metrics, text_data_prompt, text_metrics_prompt, fields=fields\n",
    "    )\n",
    "\n",
    "    print(\"\\nEnsemble:\\n\")\n",
    "    image_metrics, text_metrics_ensemble = get_model_output(\n",
    "        model, clip_model, transform, image_data, text_data_ensemble\n",
    "    )\n",
    "    compute_subgroup_correlation(\n",
    "        image_data,\n",
    "        image_metrics,\n",
    "        text_data_ensemble,\n",
    "        text_metrics_ensemble,\n",
    "        fields=fields,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waterbird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concat:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.4167 (p-value: 0.0000)\n",
      "Pearson correlation: 0.4355 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.5899 (p-value: 0.0000)\n",
      "Pearson correlation: 0.5773 (p-value: 0.0000)\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.5607 (p-value: 0.0000)\n",
      "Pearson correlation: 0.5742 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.6462 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6721 (p-value: 0.0000)\n",
      "\n",
      "Ensemble:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.6704 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6091 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.6465 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6776 (p-value: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "compute_dataset_correlation(\n",
    "    clip_model_name=\"ViT-B/32\",\n",
    "    linear_model_path=\"../pytorch_cache/iclrsubmission/models/waterbird_linear_model.pt\",\n",
    "    data_path=\"../../data/Waterbird/processed_attribute_dataset/attributes.jsonl\",\n",
    "    filter_fn=lambda i, x: x[\"attributes\"][\"split\"] == \"val\",\n",
    "    label_fn=lambda x: x[\"attributes\"][\"waterbird\"],\n",
    "    prepare_fn=prepare_waterbird,\n",
    "    fields=[\"species\", \"place\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concat:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.0801 (p-value: 0.3727)\n",
      "Pearson correlation: 0.0957 (p-value: 0.2865)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.2065 (p-value: 0.0204)\n",
      "Pearson correlation: 0.1760 (p-value: 0.0487)\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: nan (p-value: nan)\n",
      "Pearson correlation: nan (p-value: nan)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.5669 (p-value: 0.0000)\n",
      "Pearson correlation: 0.7024 (p-value: 0.0000)\n",
      "\n",
      "Ensemble:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/cs.stanford.edu/u/yuhuiz/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4878: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
      "/afs/cs.stanford.edu/u/yuhuiz/.local/lib/python3.8/site-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.3548 (p-value: 0.0000)\n",
      "Pearson correlation: 0.4141 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.5614 (p-value: 0.0000)\n",
      "Pearson correlation: 0.7227 (p-value: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "compute_dataset_correlation(\n",
    "    clip_model_name=\"ViT-B/32\",\n",
    "    linear_model_path=\"../pytorch_cache/iclrsubmission/models/fairface_linear_model.pt\",\n",
    "    data_path=\"../../data/FairFace/processed_attribute_dataset/attributes.jsonl\",\n",
    "    filter_fn=lambda i, x: x[\"attributes\"][\"split\"] == \"val\",\n",
    "    label_fn=lambda x: int(x[\"attributes\"][\"gender\"] == \"Female\"),\n",
    "    prepare_fn=prepare_fairface,\n",
    "    fields=[\"age\", \"race\", \"gender\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dSprites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concat:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.6278 (p-value: 0.0000)\n",
      "Pearson correlation: 0.6723 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.7071 (p-value: 0.0000)\n",
      "Pearson correlation: 0.7481 (p-value: 0.0000)\n",
      "\n",
      "Prompt:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.7147 (p-value: 0.0000)\n",
      "Pearson correlation: 0.7198 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.6998 (p-value: 0.0000)\n",
      "Pearson correlation: 0.7595 (p-value: 0.0000)\n",
      "\n",
      "Ensemble:\n",
      "\n",
      "Text Acc - Image Acc Correlation:\n",
      "Spearman correlation: 0.6807 (p-value: 0.0000)\n",
      "Pearson correlation: 0.8897 (p-value: 0.0000)\n",
      "Text Prob - Image Acc Correlation:\n",
      "Spearman correlation: 0.7028 (p-value: 0.0000)\n",
      "Pearson correlation: 0.7918 (p-value: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "train_idxs, val_idxs = json.load(\n",
    "    open(\"../pytorch_cache/iclrsubmission/models/dsprites_train_val_idxs_2class.json\")\n",
    ")\n",
    "\n",
    "compute_dataset_correlation(\n",
    "    clip_model_name=\"ViT-B/32\",\n",
    "    linear_model_path=\"../pytorch_cache/iclrsubmission/models/dsprites_linear_model_2class.pt\",\n",
    "    data_path=\"../../data/TriangleSquare/processed_attribute_dataset/attributes.jsonl\",\n",
    "    filter_fn=lambda i, x: i in val_idxs,\n",
    "    label_fn=lambda x: x[\"attributes\"][\"label\"],\n",
    "    prepare_fn=prepare_dspites,\n",
    "    fields=[\"color\", \"label\", \"concrete_scale\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dalle')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf49421d02fb18daac2fe024769d7389ca36bccb970e26253e571efb021ca22f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
