{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Vision Models using Language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI is becoming more pervasively used in various social contexts. As a result, it is important to understand the decisions that these models make and how they affect people. In this notebook, we will explore how to audit the decisions of a vision model using language. We will use the [DRML](https://arxiv.org/abs/1905.13677) method to audit the decisions of a vision model. We will use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset to train a vision model and the [IMDB](https://www.imdb.com/interfaces/) dataset to train a language model. We will then use the language model to audit the decisions of the vision model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download software and sampled CelebA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download our software and install dependencies\n",
    "!git clone https://github.com/yuhui-zh15/model_audit.git\n",
    "!cd model_audit\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Download the CelebA dataset and unzip it\n",
    "!wget http://cs.stanford.edu/~yuhuiz/assets/manuscripts/celeba_sampled.zip > /dev/null\n",
    "!unzip celeba_sampled.zip > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import clip\n",
    "from clip.model import CLIP\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from datasets import ImageDataset, TextDataset, create_dataloader\n",
    "from trainer import extract_features\n",
    "from models import Linear\n",
    "\n",
    "base_path = \"./celeba_sampled\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CelebA dataset and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [json.loads(line) for line in open(f\"{base_path}/attributes.jsonl\")]\n",
    "for item in data:\n",
    "    item[\"image\"] = f\"{base_path}/{item['image']}\"\n",
    "    item[\"label\"] = 1 if item[\"attributes\"][\"Wearing_Lipstick\"] == 1 else 0\n",
    "\n",
    "image_labels = torch.tensor([item[\"label\"] for item in data])\n",
    "train_idxs = [\n",
    "    i for i, item in enumerate(data) if item[\"attributes\"][\"split\"] == \"train\"\n",
    "]\n",
    "val_idxs = [i for i, item in enumerate(data) if item[\"attributes\"][\"split\"] == \"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': './celeba_sampled/images/003489.jpg',\n",
       " 'attributes': {'5_o_Clock_Shadow': -1,\n",
       "  'Arched_Eyebrows': 1,\n",
       "  'Attractive': 1,\n",
       "  'Bags_Under_Eyes': -1,\n",
       "  'Bald': -1,\n",
       "  'Bangs': -1,\n",
       "  'Big_Lips': 1,\n",
       "  'Big_Nose': -1,\n",
       "  'Black_Hair': -1,\n",
       "  'Blond_Hair': 1,\n",
       "  'Blurry': -1,\n",
       "  'Brown_Hair': -1,\n",
       "  'Bushy_Eyebrows': -1,\n",
       "  'Chubby': -1,\n",
       "  'Double_Chin': -1,\n",
       "  'Eyeglasses': -1,\n",
       "  'Goatee': -1,\n",
       "  'Gray_Hair': -1,\n",
       "  'Heavy_Makeup': 1,\n",
       "  'High_Cheekbones': 1,\n",
       "  'Male': -1,\n",
       "  'Mouth_Slightly_Open': 1,\n",
       "  'Mustache': -1,\n",
       "  'Narrow_Eyes': -1,\n",
       "  'No_Beard': 1,\n",
       "  'Oval_Face': 1,\n",
       "  'Pale_Skin': -1,\n",
       "  'Pointy_Nose': 1,\n",
       "  'Receding_Hairline': -1,\n",
       "  'Rosy_Cheeks': -1,\n",
       "  'Sideburns': -1,\n",
       "  'Smiling': 1,\n",
       "  'Straight_Hair': -1,\n",
       "  'Wavy_Hair': 1,\n",
       "  'Wearing_Earrings': -1,\n",
       "  'Wearing_Hat': -1,\n",
       "  'Wearing_Lipstick': 1,\n",
       "  'Wearing_Necklace': 1,\n",
       "  'Wearing_Necktie': -1,\n",
       "  'Young': 1,\n",
       "  'split': 'val'},\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {(1, -1): 2059, (-1, 1): 2370, (-1, -1): 560, (1, 1): 11})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = defaultdict(int)\n",
    "for item in data:\n",
    "    attributes = item[\"attributes\"]\n",
    "    counts[(attributes[\"Male\"], attributes[\"Wearing_Lipstick\"])] += 1\n",
    "\n",
    "counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract image features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for image: 100%|██████████| 5/5 [00:12<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ViT-B/32\"\n",
    "clip_model, transform = clip.load(name=model_name, device=\"cuda\")\n",
    "clip_model = clip_model.float()\n",
    "\n",
    "image_dataset = ImageDataset(data)\n",
    "image_dataloader = create_dataloader(\n",
    "    dataset=image_dataset,\n",
    "    modality=\"image\",\n",
    "    transform=transform,\n",
    "    shuffle=False,\n",
    "    batch_size=1024,\n",
    "    num_workers=16,\n",
    ")\n",
    "image_features = extract_features(\n",
    "    dataloader=image_dataloader,\n",
    "    clip_model=clip_model,\n",
    "    modality=\"image\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train visual classifier on the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss = 0.5273, train_acc = 0.9155, val_loss = 0.5287, val_acc = 0.9220\n",
      "Epoch 1: train_loss = 0.4253, train_acc = 0.9160, val_loss = 0.4268, val_acc = 0.9220\n",
      "Epoch 2: train_loss = 0.3608, train_acc = 0.9180, val_loss = 0.3624, val_acc = 0.9230\n",
      "Epoch 3: train_loss = 0.3189, train_acc = 0.9185, val_loss = 0.3202, val_acc = 0.9240\n",
      "Epoch 4: train_loss = 0.2898, train_acc = 0.9203, val_loss = 0.2909, val_acc = 0.9260\n",
      "Epoch 5: train_loss = 0.2689, train_acc = 0.9207, val_loss = 0.2698, val_acc = 0.9270\n",
      "Epoch 6: train_loss = 0.2533, train_acc = 0.9210, val_loss = 0.2541, val_acc = 0.9280\n",
      "Epoch 7: train_loss = 0.2413, train_acc = 0.9225, val_loss = 0.2421, val_acc = 0.9270\n",
      "Epoch 8: train_loss = 0.2317, train_acc = 0.9230, val_loss = 0.2325, val_acc = 0.9260\n",
      "Epoch 9: train_loss = 0.2240, train_acc = 0.9233, val_loss = 0.2250, val_acc = 0.9290\n",
      "Epoch 10: train_loss = 0.2176, train_acc = 0.9245, val_loss = 0.2187, val_acc = 0.9280\n",
      "Epoch 11: train_loss = 0.2121, train_acc = 0.9250, val_loss = 0.2134, val_acc = 0.9290\n",
      "Epoch 12: train_loss = 0.2075, train_acc = 0.9240, val_loss = 0.2089, val_acc = 0.9280\n",
      "Epoch 13: train_loss = 0.2034, train_acc = 0.9243, val_loss = 0.2051, val_acc = 0.9290\n",
      "Epoch 14: train_loss = 0.1999, train_acc = 0.9250, val_loss = 0.2019, val_acc = 0.9280\n",
      "Epoch 15: train_loss = 0.1967, train_acc = 0.9255, val_loss = 0.1991, val_acc = 0.9300\n",
      "Epoch 16: train_loss = 0.1939, train_acc = 0.9255, val_loss = 0.1966, val_acc = 0.9310\n",
      "Epoch 17: train_loss = 0.1914, train_acc = 0.9267, val_loss = 0.1945, val_acc = 0.9290\n",
      "Epoch 18: train_loss = 0.1891, train_acc = 0.9263, val_loss = 0.1925, val_acc = 0.9310\n",
      "Epoch 19: train_loss = 0.1870, train_acc = 0.9270, val_loss = 0.1908, val_acc = 0.9300\n",
      "Epoch 20: train_loss = 0.1851, train_acc = 0.9273, val_loss = 0.1891, val_acc = 0.9270\n",
      "Epoch 21: train_loss = 0.1833, train_acc = 0.9273, val_loss = 0.1878, val_acc = 0.9260\n",
      "Epoch 22: train_loss = 0.1816, train_acc = 0.9285, val_loss = 0.1865, val_acc = 0.9250\n",
      "Epoch 23: train_loss = 0.1801, train_acc = 0.9287, val_loss = 0.1854, val_acc = 0.9250\n",
      "Epoch 24: train_loss = 0.1787, train_acc = 0.9287, val_loss = 0.1846, val_acc = 0.9280\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    device: str = \"cuda\",\n",
    ") -> dict:\n",
    "    model.eval()\n",
    "    losses, preds, labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "\n",
    "            preds.extend(logits.argmax(-1).cpu().tolist())\n",
    "            labels.extend(y.cpu().tolist())\n",
    "            losses.append(loss.item())\n",
    "    preds_np, labels_np = np.array(preds), np.array(labels)\n",
    "    acc = np.mean(preds_np == labels_np)\n",
    "    mean_loss = np.mean(losses)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"loss\": mean_loss,\n",
    "        \"preds\": preds_np,\n",
    "        \"labels\": labels_np,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_image_model(\n",
    "    features: torch.Tensor,\n",
    "    labels: torch.Tensor,\n",
    "    train_idxs: list,\n",
    "    val_idxs: list,\n",
    "    n_epochs: int = 25,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 1e-3,\n",
    ") -> torch.nn.Module:\n",
    "    assert len(features) == len(\n",
    "        labels\n",
    "    ), \"Features and labels should have the same length.\"\n",
    "    features = F.normalize(features)\n",
    "\n",
    "    train_features = features[train_idxs]\n",
    "    train_labels = labels[train_idxs]\n",
    "    val_features = features[val_idxs]\n",
    "    val_labels = labels[val_idxs]\n",
    "\n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    val_dataset = TensorDataset(val_features, val_labels)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    d_model = features.shape[1]\n",
    "    n_classes = int(labels.max().item() + 1)\n",
    "    model = Linear(d_model, n_classes).cuda()\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch_idx in range(n_epochs):\n",
    "        train_one_epoch(train_dataloader, model, opt)\n",
    "        metrics_train = evaluate(train_dataloader, model)\n",
    "        metrics_val = evaluate(val_dataloader, model)\n",
    "        print(\n",
    "            f\"Epoch {epoch_idx}: train_loss = {metrics_train['loss']:.4f}, train_acc = {metrics_train['acc']:.4f}, val_loss = {metrics_val['loss']:.4f}, val_acc = {metrics_val['acc']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_image_model(image_features, image_labels, train_idxs, val_idxs)\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnose the visual classifier using the language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovering error slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a man with lipstick. [1.0, 2.5855320107126545e-09]\n",
      "a man without lipstick. [1.0, 4.570902337186489e-11]\n",
      "a woman with lipstick. [1.5590202337989467e-06, 0.9999984502792358]\n",
      "a woman without lipstick. [0.0017276920843869448, 0.9982722997665405]\n"
     ]
    }
   ],
   "source": [
    "def evaluate_language(\n",
    "    text_input: str,\n",
    "    clip_model: CLIP,\n",
    "    model: torch.nn.Module,\n",
    "    device: str = \"cuda\",\n",
    ") -> dict:\n",
    "    model.eval()\n",
    "\n",
    "    tokenized_texts = clip.tokenize([text_input]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.encode_text(tokenized_texts)\n",
    "        text_logits = model(text_features)\n",
    "        text_preds = text_logits.softmax(-1).cpu().squeeze(0).tolist()\n",
    "\n",
    "    return text_preds\n",
    "\n",
    "\n",
    "genders = [\"man\", \"woman\"]\n",
    "lipsticks = [\"with lipstick\", \"without lipstick\"]\n",
    "\n",
    "probs = []\n",
    "for gender in genders:\n",
    "    for lipstick in lipsticks:\n",
    "        text_input = f\"a {gender} {lipstick}.\"\n",
    "        text_pred = evaluate_language(text_input, clip_model, model)\n",
    "        print(text_input, text_pred)\n",
    "        probs.append(text_pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.922\n",
      "Accuracy of man with lipstick: 0.0\n",
      "Accuracy of woman without lipstick: 0.3678571428571429\n"
     ]
    }
   ],
   "source": [
    "def get_group_acc(test_idxs):\n",
    "    test_features = image_features[test_idxs]\n",
    "    test_labels = image_labels[test_idxs]\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    test_metrics = evaluate(test_dataloader, model)\n",
    "    return test_metrics[\"acc\"]\n",
    "\n",
    "\n",
    "test_idxs = [\n",
    "    i\n",
    "    for i, item in enumerate(data)\n",
    "    if item[\"attributes\"][\"Wearing_Lipstick\"] == -1 and item[\"attributes\"][\"Male\"] == -1\n",
    "]\n",
    "print(\"Overall accuracy:\", get_group_acc([i for i, item in enumerate(data)]))\n",
    "print(\n",
    "    \"Accuracy of man with lipstick:\",\n",
    "    get_group_acc(\n",
    "        [\n",
    "            i\n",
    "            for i, item in enumerate(data)\n",
    "            if item[\"attributes\"][\"Wearing_Lipstick\"] == 1\n",
    "            and item[\"attributes\"][\"Male\"] == 1\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of woman without lipstick:\",\n",
    "    get_group_acc(\n",
    "        [\n",
    "            i\n",
    "            for i, item in enumerate(data)\n",
    "            if item[\"attributes\"][\"Wearing_Lipstick\"] == -1\n",
    "            and item[\"attributes\"][\"Male\"] == -1\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying influential attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman with lipstick. [1.5590202337989467e-06, 0.9999984502792358]\n",
      "a woman without lipstick. [0.0017276920843869448, 0.9982722997665405]\n",
      "a person with lipstick. [0.09957513213157654, 0.9004248380661011]\n",
      "a person without lipstick. [0.8847416639328003, 0.1152583435177803]\n",
      "The influence of \"woman\" to \"lipstick\" = 0.4912937842309475\n"
     ]
    }
   ],
   "source": [
    "genders = [\"woman\", \"person\"]\n",
    "lipsticks = [\"with lipstick\", \"without lipstick\"]\n",
    "\n",
    "probs = []\n",
    "for gender in genders:\n",
    "    for lipstick in lipsticks:\n",
    "        text_input = f\"a {gender} {lipstick}.\"\n",
    "        text_pred = evaluate_language(text_input, clip_model, model)\n",
    "        print(text_input, text_pred)\n",
    "        probs.append(text_pred[1])\n",
    "\n",
    "print(\n",
    "    f'The influence of \"woman\" to \"lipstick\" = {((probs[0] - probs[2]) + (probs[1] - probs[3])) / 2}'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectify the visual classifier using the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1238.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_one_epoch(\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "text_inputs = [\n",
    "    \"a man with lipstick.\",\n",
    "    \"a man without lipstick.\",\n",
    "    \"a woman with lipstick.\",\n",
    "    \"a woman without lipstick.\",\n",
    "]\n",
    "text_labels = torch.tensor([1, 0, 1, 0])\n",
    "tokenized_texts = clip.tokenize(text_inputs).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(tokenized_texts)\n",
    "text_dataset = TensorDataset(text_features, text_labels)\n",
    "text_dataloader = DataLoader(text_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "for epoch in trange(1000):\n",
    "    train_one_epoch(text_dataloader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.9274\n",
      "Accuracy of man with lipstick: 0.18181818181818182\n",
      "Accuracy of woman without lipstick: 0.4589285714285714\n"
     ]
    }
   ],
   "source": [
    "def get_group_acc(test_idxs):\n",
    "    test_features = image_features[test_idxs]\n",
    "    test_labels = image_labels[test_idxs]\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    test_metrics = evaluate(test_dataloader, model)\n",
    "    return test_metrics[\"acc\"]\n",
    "\n",
    "\n",
    "test_idxs = [\n",
    "    i\n",
    "    for i, item in enumerate(data)\n",
    "    if item[\"attributes\"][\"Wearing_Lipstick\"] == -1 and item[\"attributes\"][\"Male\"] == -1\n",
    "]\n",
    "print(\"Overall accuracy:\", get_group_acc([i for i, item in enumerate(data)]))\n",
    "print(\n",
    "    \"Accuracy of man with lipstick:\",\n",
    "    get_group_acc(\n",
    "        [\n",
    "            i\n",
    "            for i, item in enumerate(data)\n",
    "            if item[\"attributes\"][\"Wearing_Lipstick\"] == 1\n",
    "            and item[\"attributes\"][\"Male\"] == 1\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "print(\n",
    "    \"Accuracy of woman without lipstick:\",\n",
    "    get_group_acc(\n",
    "        [\n",
    "            i\n",
    "            for i, item in enumerate(data)\n",
    "            if item[\"attributes\"][\"Wearing_Lipstick\"] == -1\n",
    "            and item[\"attributes\"][\"Male\"] == -1\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf49421d02fb18daac2fe024769d7389ca36bccb970e26253e571efb021ca22f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
