{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing Vision Models using Language"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI is becoming more pervasively used in various social contexts. As a result, it is important to understand the decisions that these models make and how they affect people. In this notebook, we will explore how to audit the decisions of a vision model using language. We will use the [DRML](https://arxiv.org/abs/1905.13677) method to audit the decisions of a vision model. We will use the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset to train a vision model and the [IMDB](https://www.imdb.com/interfaces/) dataset to train a language model. We will then use the language model to audit the decisions of the vision model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CelebA dataset and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for image: 100%|██████████| 12/12 [00:29<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import ImageNet\n",
    "\n",
    "from datasets import ImageDataset, TextDataset, create_dataloader\n",
    "from trainer import extract_features\n",
    "from utils import openai_imagenet_classes, openai_imagenet_template\n",
    "\n",
    "\n",
    "def filter_name(name: str) -> str:\n",
    "    return \"\".join([c for c in name.lower() if c.isalnum()])\n",
    "\n",
    "\n",
    "def extract_features_others(model_name: str, dataset: str):\n",
    "    clip_model, transform = clip.load(name=model_name, device=\"cuda\")\n",
    "    clip_model = clip_model.float()\n",
    "\n",
    "    data = [\n",
    "        json.loads(line)\n",
    "        for line in open(\n",
    "            f\"../../data/{dataset}/processed_attribute_dataset/attributes.jsonl\"\n",
    "        )\n",
    "    ]\n",
    "    for item in data:\n",
    "        item[\"label\"] = 0\n",
    "\n",
    "    image_dataset = ImageDataset(data)\n",
    "    image_dataloader = create_dataloader(\n",
    "        dataset=image_dataset,\n",
    "        modality=\"image\",\n",
    "        transform=transform,\n",
    "        shuffle=False,\n",
    "        batch_size=1024,\n",
    "        num_workers=16,\n",
    "    )\n",
    "    image_features = extract_features(\n",
    "        dataloader=image_dataloader,\n",
    "        clip_model=clip_model,\n",
    "        modality=\"image\",\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    torch.save(\n",
    "        image_features,\n",
    "        f\"{dataset.lower()}_features_{filter_name(model_name)}.pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "extract_features_others(model_name=\"ViT-B/32\", dataset=\"Waterbird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bf49421d02fb18daac2fe024769d7389ca36bccb970e26253e571efb021ca22f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
